{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c6d1c7-beca-4acf-b3c5-db26e1cfc5e4",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36ce37a-38b8-4f2e-a9ee-61e7040be5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f329a-e901-4542-851a-798bb43a9c1b",
   "metadata": {},
   "source": [
    "The cell below opens a separate window, you might need to install `all-nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aeb4e44-b930-4e13-990e-2569a9f06b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06febbf7-78fa-4eee-8892-9b8e6ccf0c19",
   "metadata": {},
   "source": [
    "## Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59fd65e9-e323-4578-bea9-3be95c3fccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another ex-Golden Stater, Paul Stankowski from Oxnard, is contending for a berth on the U.S. Ryder Cup team after winning his first PGA Tour event last year and staying within three strokes of the lead through three rounds of last month's U.S. Open. H.J. Heinz Company said it completed the sale of its Ore-Ida frozen-food business catering to the service industry to McCain Foods Ltd. for about $500 million. It's the first group action of its kind in Britain.\n"
     ]
    }
   ],
   "source": [
    "a_text = '''Another ex-Golden Stater, Paul Stankowski from Oxnard, is contending for a berth on the U.S. Ryder Cup team after winning his first PGA Tour event last year and staying within three strokes of the lead through three rounds of last month's U.S. Open. H.J. Heinz Company said it completed the sale of its Ore-Ida frozen-food business catering to the service industry to McCain Foods Ltd. for about $500 million. It's the first group action of its kind in Britain.'''\n",
    "print(a_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9d27a-e347-460a-b192-f5267570668c",
   "metadata": {},
   "source": [
    "Before the computer can apply most kinds of NLP tasks, it has to know what the separate sentences are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63305c1-e7c0-4efe-8759-ed2f6bb04ee8",
   "metadata": {},
   "source": [
    "Let's try splitting the text using a **dot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e24ddc9-3e9f-4a25-953b-cbdcfe533350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\tAnother ex-Golden Stater, Paul Stankowski from Oxnard, is contending for a berth on the U\n",
      "Sentence 2:\tS\n",
      "Sentence 3:\t Ryder Cup team after winning his first PGA Tour event last year and staying within three strokes of the lead through three rounds of last month's U\n",
      "Sentence 4:\tS\n",
      "Sentence 5:\t Open\n",
      "Sentence 6:\t H\n",
      "Sentence 7:\tJ\n",
      "Sentence 8:\t Heinz Company said it completed the sale of its Ore-Ida frozen-food business catering to the service industry to McCain Foods Ltd\n",
      "Sentence 9:\t for about $500 million\n",
      "Sentence 10:\t It's the first group action of its kind in Britain\n",
      "Sentence 11:\t\n"
     ]
    }
   ],
   "source": [
    "sentence_splitted = a_text.split('.')\n",
    "for idx, sent in enumerate(sentence_splitted, 1):\n",
    "    print(f'Sentence {idx}:\\t{sent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28779986-3495-4199-8991-69261d4029d5",
   "metadata": {},
   "source": [
    "This clearly did not work. Many abbreviations such us **U.S.** have dots in them. Luckily, you can use NLTK to split a text into sentences more reliably. Let's see how it performs on our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae91cf5-57ce-4237-b77c-856281a21108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd85a45-2e5a-4f15-837b-1a044b55c442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\tAnother ex-Golden Stater, Paul Stankowski from Oxnard, is contending for a berth on the U.S. Ryder Cup team after winning his first PGA Tour event last year and staying within three strokes of the lead through three rounds of last month's U.S. Open.\n",
      "Sentence 2:\tH.J.\n",
      "Sentence 3:\tHeinz Company said it completed the sale of its Ore-Ida frozen-food business catering to the service industry to McCain Foods Ltd. for about $500 million.\n",
      "Sentence 4:\tIt's the first group action of its kind in Britain.\n"
     ]
    }
   ],
   "source": [
    "nltk_sentence_splitted = sent_tokenize(a_text)\n",
    "for idx, sent in enumerate(nltk_sentence_splitted, 1):\n",
    "    print(f'Sentence {idx}:\\t{sent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7df168-e920-484e-807f-5629a390bbe3",
   "metadata": {},
   "source": [
    "Interestingly, the model is not perfect. It correctly determines that *U.S. Ryder Cup* is not the end of the sentence. However, it states that **H.J.** is the end of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08634d-5bd1-4a00-9102-b087dea572e2",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794838f9-7a87-4a91-b701-a9f8bb94468b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's the first group action of its kind in Britain.\n"
     ]
    }
   ],
   "source": [
    "example_sentence = nltk_sentence_splitted[-1]\n",
    "print(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b6794-3746-4c3e-9693-badbf2f5b264",
   "metadata": {},
   "source": [
    "The most naive way to apply tokenization is to split a text using spaces. Let's try this. Please run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93cddc70-c0f7-4a79-a026-9baf359febec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", 'the', 'first', 'group', 'action', 'of', 'its', 'kind', 'in', 'Britain.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_using_spaces = example_sentence.split(' ')\n",
    "print(tokenized_using_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c4e4f-e57c-4333-ba08-7c2bbe3fbbb0",
   "metadata": {},
   "source": [
    "Tokenizing using spaces does actually work for most tokens. However, it does not work for expressions such as **It's** in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d2679-1d12-4de6-b3f9-bd3f9b20c9a6",
   "metadata": {},
   "source": [
    "Let's try a real tokenizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb38f6c7-56c0-43cd-8669-7e13f3dd04bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'the', 'first', 'group', 'action', 'of', 'its', 'kind', 'in', 'Britain', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_using_tokenizer = nltk.word_tokenize(example_sentence)\n",
    "print(tokenized_using_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fd84fc-65e0-4a49-ac5b-8cbcbe99284a",
   "metadata": {},
   "source": [
    "Please note that **It's** is now correctly tokenized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b20f0-be8b-4bac-8c81-f3e07fae54ba",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb3d91-8567-4461-905d-a750591284f6",
   "metadata": {},
   "source": [
    "NLTK has various modules for stripping inflection of words (stemming) or finding the lemma (the form you can find in a dictionary). Below is a script to stem and lemmatize the words in a text example after tokenizing the text using different modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad1ba85b-6cf8-4616-b496-d680315b3fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter\n",
      "['anoth', 'ex-golden', 'stater', ',', 'paul', 'stankowski', 'from', 'oxnard', ',', 'is', 'contend', 'for', 'a', 'berth', 'on', 'the', 'u.s.', 'ryder', 'cup', 'team', 'after', 'win', 'hi', 'first', 'pga', 'tour', 'event', 'last', 'year', 'and', 'stay', 'within', 'three', 'stroke', 'of', 'the', 'lead', 'through', 'three', 'round', 'of', 'last', 'month', \"'s\", 'u.s.', 'open', '.', 'h.j', '.', 'heinz', 'compani', 'said', 'it', 'complet', 'the', 'sale', 'of', 'it', 'ore-ida', 'frozen-food', 'busi', 'cater', 'to', 'the', 'servic', 'industri', 'to', 'mccain', 'food', 'ltd.', 'for', 'about', '$', '500', 'million', '.', 'it', \"'s\", 'the', 'first', 'group', 'action', 'of', 'it', 'kind', 'in', 'britain', '.']\n",
      "\n",
      "Snowball\n",
      "['anoth', 'ex-golden', 'stater', ',', 'paul', 'stankowski', 'from', 'oxnard', ',', 'is', 'contend', 'for', 'a', 'berth', 'on', 'the', 'u.s.', 'ryder', 'cup', 'team', 'after', 'win', 'his', 'first', 'pga', 'tour', 'event', 'last', 'year', 'and', 'stay', 'within', 'three', 'stroke', 'of', 'the', 'lead', 'through', 'three', 'round', 'of', 'last', 'month', \"'s\", 'u.s.', 'open', '.', 'h.j', '.', 'heinz', 'compani', 'said', 'it', 'complet', 'the', 'sale', 'of', 'it', 'ore-ida', 'frozen-food', 'busi', 'cater', 'to', 'the', 'servic', 'industri', 'to', 'mccain', 'food', 'ltd.', 'for', 'about', '$', '500', 'million', '.', 'it', \"'s\", 'the', 'first', 'group', 'action', 'of', 'it', 'kind', 'in', 'britain', '.']\n",
      "\n",
      "Wordnet\n",
      "['Another', 'ex-Golden', 'Stater', ',', 'Paul', 'Stankowski', 'from', 'Oxnard', ',', 'is', 'contending', 'for', 'a', 'berth', 'on', 'the', 'U.S.', 'Ryder', 'Cup', 'team', 'after', 'winning', 'his', 'first', 'PGA', 'Tour', 'event', 'last', 'year', 'and', 'staying', 'within', 'three', 'stroke', 'of', 'the', 'lead', 'through', 'three', 'round', 'of', 'last', 'month', \"'s\", 'U.S.', 'Open', '.', 'H.J', '.', 'Heinz', 'Company', 'said', 'it', 'completed', 'the', 'sale', 'of', 'it', 'Ore-Ida', 'frozen-food', 'business', 'catering', 'to', 'the', 'service', 'industry', 'to', 'McCain', 'Foods', 'Ltd.', 'for', 'about', '$', '500', 'million', '.', 'It', \"'s\", 'the', 'first', 'group', 'action', 'of', 'it', 'kind', 'in', 'Britain', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "tokens = nltk.word_tokenize(a_text)\n",
    "\n",
    "porterlemmas = []\n",
    "wordnetlemmas = []\n",
    "snowballlemmas = []\n",
    "\n",
    "for token in tokens:\n",
    "    porterlemmas.append(porter.stem(token))\n",
    "    snowballlemmas.append(snowball.stem(token))\n",
    "    wordnetlemmas.append(wordnet.lemmatize(token))\n",
    "\n",
    "print('Porter')\n",
    "print(porterlemmas)\n",
    "print('\\nSnowball')\n",
    "print(snowballlemmas)\n",
    "print('\\nWordnet')\n",
    "print(wordnetlemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81320810-2d75-4a90-8d5f-f20b0e3dd3e1",
   "metadata": {},
   "source": [
    "What differences do you see between the three lists?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4121d99-3bb9-43f8-99ec-37322059b120",
   "metadata": {},
   "source": [
    "## Part of Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93830f3-0a14-4d69-831b-975e5ae06e44",
   "metadata": {},
   "source": [
    "a useful next step is to determine the part of speech of each token.\n",
    "The part of speech is the syntactic category of a token. \n",
    "\n",
    "| the | red   | clown  | behaved  | weirdly  |\n",
    "|---|---|---|---|---|\n",
    "| determiner | adjective | noun | verb | adverb |\n",
    "\n",
    "We can replace tokens with another token with the same part of speech, and the sentence would still be grammatical. For example:\n",
    "* The **blue** clown behaved weirdly.\n",
    "* The red **cow** behaved weirdly.\n",
    "* The red clown **walked** weirdly.\n",
    "\n",
    "NLTK also provides a method to automatically tag each token in a text with a part of speech tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d19ed5d3-dacb-4eec-ad9c-4d65b186cffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " (\"'ll\", 'MD'),\n",
       " ('refuse', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['I', \"'ll\", 'refuse', 'to', 'permit', 'you', 'to', 'obtain', 'the', 'refuse', 'permit', '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f3b72-8779-470d-9f4b-77a85f727070",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d804c78f-1630-445a-ab7d-721fde881f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = \"I'll refuse to permit you to obtain the refuse permit.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13c3ec-6643-42ad-b082-8d90aa341b68",
   "metadata": {},
   "source": [
    "As a first step, we obtain the vocabulary — all individual tokens in the corpus, which in our case is just one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b16ce43f-0aff-4be4-9fb7-488601258c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'ll\", 'refuse', 'to', 'permit', 'you', 'obtain', 'the', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent = nltk.word_tokenize(example_sent)\n",
    "vocabulary = []\n",
    "\n",
    "for token in tokenized_sent:\n",
    "    if token not in vocabulary:\n",
    "        vocabulary.append(token)\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6701b4-8286-4a92-9e0f-e882ac8e7a97",
   "metadata": {},
   "source": [
    "Note that the punctuation mark `.` is part of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4520c-64bc-45f2-b1ba-54cadc5b2b4f",
   "metadata": {},
   "source": [
    "Next, we vectorize the sentence by counting how often each token appears in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c857050-1c55-4013-996b-756978c22db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 2, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "bow_vect = []\n",
    "\n",
    "for token in vocabulary:\n",
    "    token_count_in_sent = tokenized_sent.count(token)\n",
    "    bow_vect.append(token_count_in_sent)\n",
    "\n",
    "print(bow_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f054354-4338-4a6e-9f5d-d728105be238",
   "metadata": {},
   "source": [
    "In this representation, the first 1 indicates that the token *I* appears once in the sentence. The 2 in the third position indicates that the third token in our vocabulary, *refuse*, appears twice. Let's now vectorize a different sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "081cd1af-5b61-4c76-b604-a59f271b61a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "different_sent = \"I'll permit the refuse.\"\n",
    "different_tokenized_sent = nltk.word_tokenize(different_sent)\n",
    "\n",
    "different_bow_vect = []\n",
    "\n",
    "for token in vocabulary:\n",
    "    token_count_in_sent = different_tokenized_sent.count(token)\n",
    "    different_bow_vect.append(token_count_in_sent)\n",
    "\n",
    "print(different_bow_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e73ac-17c2-40a4-9dc4-860cc80af664",
   "metadata": {},
   "source": [
    "What would the representation for *I'll refuse the permit.* look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00198e-48e4-4342-93df-bf9b9e27220d",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ae2dc-49c6-4b14-a58a-2bc87fcd273b",
   "metadata": {},
   "source": [
    "One way to overcome this shortcoming is to look at a text not as a bag of words, but rather a collection of n-grams. NLTK has a module for this as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70b57255-5f0a-4e66-b83e-622280a1c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d5a971-a5a6-4ae8-9021-6f55468d6db3",
   "metadata": {},
   "source": [
    "For reference, here is the example sentence again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b367351a-2c1b-4281-964e-b65b9cd5e863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'ll\", 'refuse', 'to', 'permit', 'you', 'to', 'obtain', 'the', 'refuse', 'permit', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "760e4e17-4d34-4897-8854-cbee6112153d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('I', \"'ll\"), (\"'ll\", 'refuse'), ('refuse', 'to'), ('to', 'permit'), ('permit', 'you'), ('you', 'to'), ('to', 'obtain'), ('obtain', 'the'), ('the', 'refuse'), ('refuse', 'permit'), ('permit', '.')]\n",
      "\n",
      "Trigrams: [('I', \"'ll\", 'refuse'), (\"'ll\", 'refuse', 'to'), ('refuse', 'to', 'permit'), ('to', 'permit', 'you'), ('permit', 'you', 'to'), ('you', 'to', 'obtain'), ('to', 'obtain', 'the'), ('obtain', 'the', 'refuse'), ('the', 'refuse', 'permit'), ('refuse', 'permit', '.')]\n"
     ]
    }
   ],
   "source": [
    "bigrams = list(ngrams(tokenized_sent, 2))\n",
    "trigrams = list(ngrams(tokenized_sent, 3))\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"\\nTrigrams:\", trigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
